{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# External modules\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchvision.models import resnet18\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from huggingface_hub import from_pretrained_keras\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# fixing paths\n",
    "project_root = os.path.abspath(\"..\")\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# owned modules\n",
    "from src.datasets import SCINDataset\n",
    "from src.models import FeatureExtractor, ClinicalOutcomePredictor, Adversary\n",
    "from src.utils import custom_collate_fn\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#models\n",
    "google_vit_huge = \"google/vit-huge-patch14-224-in21k\"\n",
    "google_derm = \"google/derm-foundation\"\n",
    "\n",
    "# seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),     \n",
    "])\n",
    "\n",
    "protected_attribute = 'combined_race'\n",
    "dataset = SCINDataset(\n",
    "    root_dir=\"../data/external/scin/dataset\",\n",
    "    labels_csv=\"scin_labels.csv\",\n",
    "    cases_csv=\"scin_cases.csv\",\n",
    "    transform=transform,\n",
    "    protected_attr=protected_attribute\n",
    ")\n",
    "\n",
    "num_classes = len(dataset.label_encoder.classes_)\n",
    "num_protected_attributes = len(dataset.protected_label_encoder.classes_)\n",
    "\n",
    "indices = list(range(len(dataset)))\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "train_data = Subset(dataset, train_indices)\n",
    "val_data = Subset(dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating dataset train/val split\n",
    "\n",
    "print(\"Number of training samples:\", len(train_data))\n",
    "print(\"Number of validation samples:\", len(val_data))\n",
    "print(len(dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting outcomes only (Google ViT Huge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "\n",
    "batch_size = 16\n",
    "lr = 1e-6\n",
    "num_epochs = 20\n",
    "# lambda_ = 1\n",
    "dropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition/parameters\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(google_vit_huge).to(device)\n",
    "model.classifier = torch.nn.Linear(model.config.hidden_size, num_classes)\n",
    "\n",
    "model.vit.embeddings.dropout.p = dropout\n",
    "for layer in model.vit.encoder.layer:\n",
    "\tlayer.attention.attention.dropout.p = dropout\n",
    "\tlayer.attention.output.dropout.p = dropout\n",
    "\tlayer.output.dropout.p = dropout\n",
    "\n",
    "for param in model.parameters():\n",
    "\tparam.requires_grad = False\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "\tparam.requires_grad = True\n",
    " \n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Overall Training Progress\"):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # TRAINING\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    train_epoch_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch [{epoch+1}/{num_epochs}]\"):\n",
    "        images, outcomes = [x.to(device) for x in batch[:2]]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outcomes_pred = model(images).logits\n",
    "        loss = criterion(outcomes_pred, outcomes)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted = torch.argmax(outcomes_pred, dim=1)\n",
    "        total_samples += outcomes.size(0)\n",
    "        correct += (predicted == outcomes).sum().item()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "    epoch_accuracy = 100 * correct / total_samples\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "\n",
    "    train_time = time.time() - train_epoch_time\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, \"\n",
    "          f\"Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "    # VALIDATION    \n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    val_epoch_time = time.time()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Validation Epoch [{epoch+1}/{num_epochs}]\"):\n",
    "            images, outcomes = [x.to(device) for x in batch[:2]]\n",
    "\n",
    "            outcomes_pred = model(images).logits\n",
    "            \n",
    "            predicted = torch.argmax(outcomes_pred, dim=1)\n",
    "            val_total += outcomes.size(0)\n",
    "            val_correct += (predicted == outcomes).sum().item()\n",
    "\n",
    "    val_epoch_accuracy = 100 * val_correct / val_total\n",
    "    val_accuracy.append(val_epoch_accuracy)\n",
    "\n",
    "    val_time = time.time() - val_epoch_time    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Accuracy: {val_epoch_accuracy:.2f}%\")\n",
    "        \n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed in {epoch_time:.2f}s \"\n",
    "          f\"(Train: {train_time:.2f}s, Val: {val_time:.2f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting race only (Google ViT Huge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "\n",
    "batch_size = 16\n",
    "lr = 1e-6\n",
    "num_epochs = 20\n",
    "# lambda_ = 1\n",
    "dropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-huge-patch14-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model definition/parameters\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(google_vit_huge).to(device)\n",
    "model.classifier = torch.nn.Linear(model.config.hidden_size, num_protected_attributes)\n",
    "\n",
    "model.vit.embeddings.dropout.p = dropout\n",
    "for layer in model.vit.encoder.layer:\n",
    "\tlayer.attention.attention.dropout.p = dropout\n",
    "\tlayer.attention.output.dropout.p = dropout\n",
    "\tlayer.output.dropout.p = dropout\n",
    "\n",
    "for param in model.parameters():\n",
    "\tparam.requires_grad = False\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "\tparam.requires_grad = True\n",
    " \n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Overall Training Progress\"):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # TRAINING\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    train_epoch_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch [{epoch+1}/{num_epochs}]\"):\n",
    "        images, _, attributes = [x.to(device) for x in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        attributes_pred = model(images).logits\n",
    "        loss = criterion(attributes_pred, attributes)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted = torch.argmax(attributes_pred, dim=1)\n",
    "        total_samples += attributes.size(0)\n",
    "        correct += (predicted == attributes).sum().item()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "    epoch_accuracy = 100 * correct / total_samples\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "\n",
    "    train_time = time.time() - train_epoch_time\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, \"\n",
    "          f\"Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "    # VALIDATION    \n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    val_epoch_time = time.time()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Validation Epoch [{epoch+1}/{num_epochs}]\"):\n",
    "            images, _, attributes = [x.to(device) for x in batch]\n",
    "\n",
    "            attributes_pred = model(images).logits\n",
    "            \n",
    "            predicted = torch.argmax(attributes_pred, dim=1)\n",
    "            val_total += attributes.size(0)\n",
    "            val_correct += (predicted == attributes).sum().item()\n",
    "\n",
    "    val_epoch_accuracy = 100 * val_correct / val_total\n",
    "    val_accuracy.append(val_epoch_accuracy)\n",
    "\n",
    "    val_time = time.time() - val_epoch_time    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Accuracy: {val_epoch_accuracy:.2f}%\")\n",
    "        \n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed in {epoch_time:.2f}s \"\n",
    "          f\"(Train: {train_time:.2f}s, Val: {val_time:.2f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "batch_size = 64\n",
    "lr = 1e-6\n",
    "num_epochs = 20\n",
    "lambda_ = 1\n",
    "dropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition/parameters\n",
    "model = AutoModelForImageClassification.from_pretrained(google_vit_huge).to(device)\n",
    "model.classifier = torch.nn.Identity()\n",
    "\n",
    "print(model)\n",
    "extractor = FeatureExtractor(model).to(device)\n",
    "predictor = ClinicalOutcomePredictor(embedding_dim=extractor.embedding_dim, num_outcomes=num_classes).to(device)\n",
    "adversary = Adversary(embedding_dim=extractor.embedding_dim, num_protected_attributes=num_protected_attributes).to(device)\n",
    "\n",
    "primary_model = {'extractor': extractor, 'predictor': predictor}\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in predictor.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "criterion_primary = torch.nn.CrossEntropyLoss()\n",
    "criterion_adversary = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(extractor.parameters()) + \n",
    "    list(predictor.parameters()) + \n",
    "    list(adversary.parameters()), lr=lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Overall Training Progress\"):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # TRAINING\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    primary_model['extractor'].train()\n",
    "    primary_model['predictor'].train()\n",
    "    adversary.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_primary_loss = 0.0\n",
    "    total_adversary_loss = 0.0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    train_epoch_time = time.time()\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch [{epoch+1}/{num_epochs}]\"):\n",
    "        images, outcomes, protected_attributes = [x.to(device) for x in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        embeddings = primary_model['extractor'](images)\n",
    "        outcomes_pred = primary_model['predictor'](embeddings)\n",
    "        protected_pred = adversary(embeddings)\n",
    "\n",
    "        loss_primary = criterion_primary(outcomes_pred, outcomes)\n",
    "        loss_adversary = criterion_adversary(protected_pred, protected_attributes)\n",
    "        loss = loss_primary - lambda_ * loss_adversary\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted = torch.argmax(outcomes_pred, dim=1)\n",
    "        total_samples += outcomes.size(0)\n",
    "        correct += (predicted == outcomes).sum().item()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_primary_loss += loss_primary.item()\n",
    "        total_adversary_loss += loss_adversary.item()\n",
    "\n",
    "    epoch_accuracy = 100 * correct / total_samples\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "\n",
    "    train_time = time.time() - train_epoch_time\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, \"\n",
    "          f\"Primary Loss: {total_primary_loss:.4f}, Adversary Loss: {total_adversary_loss:.4f}, \"\n",
    "          f\"Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "    # VALIDATION    \n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    val_epoch_time = time.time()\n",
    "    primary_model['extractor'].eval()\n",
    "    primary_model['predictor'].eval()\n",
    "    adversary.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Validation Epoch [{epoch+1}/{num_epochs}]\"):\n",
    "            images, outcomes, protected_attributes = [x.to(device) for x in batch]\n",
    "\n",
    "            embeddings = primary_model['extractor'](images)\n",
    "            outcomes_pred = primary_model['predictor'](embeddings)\n",
    "\n",
    "            predicted = torch.argmax(outcomes_pred, dim=1)\n",
    "            val_total += outcomes.size(0)\n",
    "            val_correct += (predicted == outcomes).sum().item()\n",
    "\n",
    "    val_epoch_accuracy = 100 * val_correct / val_total\n",
    "    val_accuracy.append(val_epoch_accuracy)\n",
    "\n",
    "    val_time = time.time() - val_epoch_time    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Accuracy: {val_epoch_accuracy:.2f}%\")\n",
    "        \n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed in {epoch_time:.2f}s \"\n",
    "          f\"(Train: {train_time:.2f}s, Val: {val_time:.2f}s)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
